{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c8074c",
   "metadata": {},
   "source": [
    "Examen PySpark\n",
    "Instrucciones: Lea cuidadosamente las preguntas, escriba el código correspondiente y ejecútelo para mostrar sus resultados.\n",
    "\n",
    "Importante: Todos los ejercicios deberán realizarse con funciones de NumPy, Pandas o PySpark (no podrán crearse vistas temporales para realizarse en SQL, salvo que se indique lo contrario)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd295e3",
   "metadata": {},
   "source": [
    "1.1 Utilizando NumPy, construya un arreglo con 50 elementos aleatorios distribuidos de forma normal con media 50 y desviación estándar 10. Imprima el arreglo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8198cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c906e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.04316237 43.72370767 62.83363972 59.23240647 54.45381436 56.08564635\n",
      " 56.92165967 40.85530363 47.01727796 52.92960562 56.67032375 41.94263761\n",
      " 63.18626006 35.82079251 56.97839199 50.53012189 54.64222832 39.93023964\n",
      " 48.95581834 64.80919255 49.30900384 44.1557694  34.94099168 62.27771462\n",
      " 61.8872053  52.00162512 61.12248833 37.28464461 60.57197059 57.96061479\n",
      " 77.56091868 62.89064105 48.64702223 44.09035403 40.14135355 58.34462852\n",
      " 56.68706383 60.60498466 52.31258585 59.62115622 69.83035174 49.37941611\n",
      " 51.23289531 40.80846773 51.80886103 66.88520535 46.26600815 62.25856068\n",
      " 37.39785911 61.05474058]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.normal(50, 10, 50)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2decf5c",
   "metadata": {},
   "source": [
    "1.2. Construya el objeto de Spark (Core) que le permita trabajar con objetos RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84939af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "sc = pyspark.SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a3c3b",
   "metadata": {},
   "source": [
    "1.3. Convierta el arreglo de NumPy a un RDD con 2 particiones. Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d99573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50.04316237493861,\n",
       " 43.723707667524536,\n",
       " 62.83363972365511,\n",
       " 59.232406469864344,\n",
       " 54.45381435656503]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd = sc.parallelize(a, 2)\n",
    "myrdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93394a8e",
   "metadata": {},
   "source": [
    "1.4. Suponiendo que los datos de la lista miden grados Fahrenheit, aplique una función lambda al RDD que convierta las mediciones a grados Centígrados. Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a978f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.023979097188118,\n",
       " 6.51317092640252,\n",
       " 17.12979984647506,\n",
       " 15.129114705480191,\n",
       " 12.474341309202796]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centigrados = myrdd.map(lambda F: ((F - 32) * 5 / 9))\n",
    "centigrados.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a197a038",
   "metadata": {},
   "source": [
    "1.5. Utilice una función Lambda para mostrar únicamente las temperaturas mayores a 15 grados Centigrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f014ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.12979984647506,\n",
       " 15.129114705480191,\n",
       " 17.325700030770577,\n",
       " 18.227329195564547,\n",
       " 16.820952565362514,\n",
       " 16.604002946445892,\n",
       " 16.17916018212483,\n",
       " 15.873316997159456,\n",
       " 25.31162149082154,\n",
       " 17.161467249737736,\n",
       " 15.891658144951244,\n",
       " 15.345086787347526,\n",
       " 21.01686207868806,\n",
       " 19.38066963880271,\n",
       " 16.810311486728235,\n",
       " 16.141522543877553]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centigrados.filter(lambda x: x > 15).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91d853",
   "metadata": {},
   "source": [
    "1.6. Calcule la temperatura media en grados Centígrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5122638a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.743303702068593"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centigrados.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28a7c1",
   "metadata": {},
   "source": [
    "1.7. Obtenga las 3 temperaturas más altas en grados Centígrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7939fb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25.31162149082154, 21.01686207868806, 19.38066963880271]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centigrados.sortBy(lambda x:x, ascending=False).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d65c3",
   "metadata": {},
   "source": [
    "Bloque 2: Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c4949",
   "metadata": {},
   "source": [
    "2.1. Utilizando Numpy, construya un arreglo con 50 números enteros entre 1 y 3 (1 y 3 incluidos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1c8a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 1 2 2 1 2 3 2 3 2 3 2 1 3 2 1 2 2 2 1 3 1 1 3 1 1 2 2 2 3 1 2 1 1 2\n",
      " 3 1 2 2 2 3 2 2 3 3 2 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 1, 2, 2, 1, 2, 3, 2, 3, 2, 3, 2, 1, 3, 2, 1, 2, 2, 2, 1,\n",
       "       3, 1, 1, 3, 1, 1, 2, 2, 2, 3, 1, 2, 1, 1, 2, 3, 1, 2, 2, 2, 3, 2,\n",
       "       2, 3, 3, 2, 1, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randint(1, 4, 50)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548192e",
   "metadata": {},
   "source": [
    "2.2. Construya un dataframe en Pandas utilizando los arreglos de 2.1 y 1.1. Asigne los nombres \"dia\" y \"temp\". Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79bfc7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dia</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>50.043162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>43.723708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>62.833640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>59.232406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>54.453814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dia       temp\n",
       "0    1  50.043162\n",
       "1    2  43.723708\n",
       "2    3  62.833640\n",
       "3    1  59.232406\n",
       "4    2  54.453814"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_data = np.array([b, a])\n",
    "df = pd.DataFrame(numpy_data.T, columns=[\"dia\", \"temp\"])\n",
    "df.dia = df.dia.astype(int)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aebe97",
   "metadata": {},
   "source": [
    "2.3. Construya el objeto de Spark (SQL) que le permita trabajar con los dataframes de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48e7cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "    master(\"local[2]\").\\\n",
    "    appName(\"challenge\").\\\n",
    "    config('spark.default.parallelism',2).\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666bab8",
   "metadata": {},
   "source": [
    "2.4. Convierta el dataframe de Pandas a un dataframe de Spark, definiendo explícitamente el esquema/estructura (utilice el tipo entero para el día y el tipo doble para la temperatura). Muestre los primeros 5 registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64b3f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|dia|              temp|\n",
      "+---+------------------+\n",
      "|  1| 50.04316237493861|\n",
      "|  2|43.723707667524536|\n",
      "|  3| 62.83363972365511|\n",
      "|  1|59.232406469864344|\n",
      "|  2| 54.45381435656503|\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StructType, StructField, DoubleType\n",
    " \n",
    "miesquema = StructType([\n",
    "     StructField(\"dia\",IntegerType(),True),\n",
    "     StructField(\"temp\",DoubleType(),True)])\n",
    "\n",
    "df_spark = spark.createDataFrame(df, miesquema)\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc43c2",
   "metadata": {},
   "source": [
    "2.5. Partiendo del dataframe en Spark, construya un dataframe con el promedio de temperatura agrupado por día. El dataframe deberá contener únicamente las columnas \"dia\" y \"temp_prom\" (con esos nombres). Muestre la tabla resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7976ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "|dia|        temp_prom|\n",
      "+---+-----------------+\n",
      "|  1|51.11180877756445|\n",
      "|  3|54.58833122497515|\n",
      "|  2|53.70261855752193|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = df_spark.groupBy(\"dia\").agg({'temp': 'mean'}).toDF(\"dia\", \"temp_prom\")\n",
    "gb.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c471a",
   "metadata": {},
   "source": [
    "2.6. Repita el ejercicio anterior registrando una vista temporal y ejecutando el código SQL correspondiente. Muestre la tabla resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e98a5df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "|dia|        temp_prom|\n",
      "+---+-----------------+\n",
      "|  1|51.11180877756445|\n",
      "|  3|54.58833122497515|\n",
      "|  2|53.70261855752193|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.createOrReplaceTempView(\"temperaturas\")\n",
    "spark.sql(\"select dia, avg(temp) as temp_prom FROM temperaturas GROUP BY dia\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd2e26",
   "metadata": {},
   "source": [
    "2.7. Combine los valores del dataframe anterior con el original. El dataframe resultante no deberá contener columnas repetidas y tendrá que estar ordenado de forma ascendente por día y temperatura. Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fde00c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+\n",
      "|dia|              temp|        temp_prom|\n",
      "+---+------------------+-----------------+\n",
      "|  1| 37.28464461164833|51.11180877756445|\n",
      "|  1|37.397859107548456|51.11180877756445|\n",
      "|  1|39.930239643141576|51.11180877756445|\n",
      "|  1| 40.14135355104299|51.11180877756445|\n",
      "|  1|44.155769403252734|51.11180877756445|\n",
      "+---+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = df_spark.join(gb, \"dia\").orderBy(\"dia\", \"temp\")\n",
    "joined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0c196",
   "metadata": {},
   "source": [
    "2.8. Añada una columna adicicional con la diferencia entre la temperatura y su media. Asigne el nombre \"resid\". Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32271afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+-------------------+\n",
      "|dia|              temp|        temp_prom|              resid|\n",
      "+---+------------------+-----------------+-------------------+\n",
      "|  1| 37.28464461164833|51.11180877756445|-13.827164165916116|\n",
      "|  1|37.397859107548456|51.11180877756445|-13.713949670015992|\n",
      "|  1|39.930239643141576|51.11180877756445|-11.181569134422872|\n",
      "|  1| 40.14135355104299|51.11180877756445|-10.970455226521459|\n",
      "|  1|44.155769403252734|51.11180877756445| -6.956039374311715|\n",
      "+---+------------------+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "residuos = joined.withColumn(\"resid\", joined.temp- joined.temp_prom)\n",
    "residuos.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3824167",
   "metadata": {},
   "source": [
    "2.9. Construya un dataframe con todos los registros que posean residuales negativos. Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f8b63cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+-------------------+\n",
      "|dia|              temp|        temp_prom|              resid|\n",
      "+---+------------------+-----------------+-------------------+\n",
      "|  1| 37.28464461164833|51.11180877756445|-13.827164165916116|\n",
      "|  1|37.397859107548456|51.11180877756445|-13.713949670015992|\n",
      "|  1|39.930239643141576|51.11180877756445|-11.181569134422872|\n",
      "|  1| 40.14135355104299|51.11180877756445|-10.970455226521459|\n",
      "|  1|44.155769403252734|51.11180877756445| -6.956039374311715|\n",
      "+---+------------------+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "residuos_filtered = residuos.filter(residuos.resid < 0)\n",
    "residuos_filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae6510",
   "metadata": {},
   "source": [
    "2.10. Guarde el dataframe resultante en formato JSON. En caso de que el archivo ya exista, deberá sobreescribirse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb426dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuos_filtered.write.mode( \"overwrite\" ).json( \"solution\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90924d2c",
   "metadata": {},
   "source": [
    "Bloque 3: Spark MLlib\n",
    "En esta sección se evalúan los conocimientos de Spark MLlib. Si bien son necesarios los conociemientos en Machine Learning, el candidato no será evaluado por la calidad del modelo producido.\n",
    "3.1. Cargue los datos del archivo 'data.csv'.\n",
    "\n",
    "3.2. Realicé un análisis exploratorio preliminar de los datos (estadísticos básicos de las columnas).\n",
    "\n",
    "3.3. Obtenga el conjunto de datos con el vector de variables independientes y la variable dependiente (churn). Por simplicidad, es suficiente que seleccione únicamente las variables numéricas. Muestre los primeros 5 elementos.\n",
    "\n",
    "3.4. Realicé la separación en los conjuntos de entrenamiento y prueba con una proporción 70-30. Muestre los primeros 5 elementos de cada conjunto de datos.\n",
    "\n",
    "3.5. Ajuste un modelo de regresión logística con los hiperparámetros por defecto. Muestre los estadísticos descriptivos de las predicciones contenidas en el resumen del modelo.\n",
    "\n",
    "3.6. Evalúe los resultados en el conjunto de prueba. Muestre las primeras 5 predicciones.\n",
    "\n",
    "3.7. Para evaluar el desempeño del modelo, obtenga el valor del indicador auROC (área debajo de la curva ROC).\n",
    "\n",
    "3.8. Cargue los datos del archivo 'data_new.csv' y obtenga las predicciones sobre ese conjunto de datos utilizando los objetos construidos previamente. Muestre los primeros 5 elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b992d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|       Onboard_date|            Location|             Company|              Churn|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          900|              900|              900|               900|              900|               900|                900|                 900|                 900|                900|\n",
      "|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|               null|                null|                null|0.16666666666666666|\n",
      "| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|               null|                null|                null| 0.3728852122772358|\n",
      "|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|2006-01-02 04:16:13|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n",
      "|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|2016-12-28 04:07:38|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.read.csv('data.csv',header=True,inferSchema=True, sep=',')\n",
    "training.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d3a4bead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+\n",
      "|              Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|Independent Features|\n",
      "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+\n",
      "|   Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|[42.0,11066.8,0.0...|\n",
      "|      Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|[41.0,11916.22,0....|\n",
      "|        Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|[38.0,12884.75,0....|\n",
      "|      Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|[42.0,8010.76,0.0...|\n",
      "|     Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|[37.0,9191.58,0.0...|\n",
      "|   Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|2009-03-03 23:13:37|6187 Olson Mounta...|        Kelly-Warren|    1|[48.0,10356.02,0....|\n",
      "|        Eric Butler|44.0|      11331.58|              1| 5.23|     11.0|2016-12-05 03:35:43|4846 Savannah Roa...|   Reynolds-Sheppard|    1|[44.0,11331.58,1....|\n",
      "|      Zachary Walsh|32.0|       9885.12|              1| 6.92|      9.0|2006-03-09 14:50:20|25271 Roy Express...|          Singh-Cole|    1|[32.0,9885.12,1.0...|\n",
      "|        Ashlee Carr|43.0|       14062.6|              1| 5.46|     11.0|2011-09-29 05:47:23|3725 Caroline Str...|           Lopez PLC|    1|[43.0,14062.6,1.0...|\n",
      "|     Jennifer Lynch|40.0|       8066.94|              1| 7.11|     11.0|2006-03-28 15:42:45|363 Sandra Lodge ...|       Reed-Martinez|    1|[40.0,8066.94,1.0...|\n",
      "|       Paula Harris|30.0|      11575.37|              1| 5.22|      8.0|2016-11-13 13:13:01|Unit 8120 Box 916...|Briggs, Lamb and ...|    1|[30.0,11575.37,1....|\n",
      "|     Bruce Phillips|45.0|       8771.02|              1| 6.64|     11.0|2015-05-28 12:14:03|Unit 1895 Box 094...|    Figueroa-Maynard|    1|[45.0,8771.02,1.0...|\n",
      "|       Craig Garner|45.0|       8988.67|              1| 4.84|     11.0|2011-02-16 08:10:47|897 Kelley Overpa...|     Abbott-Thompson|    1|[45.0,8988.67,1.0...|\n",
      "|       Nicole Olson|40.0|       8283.32|              1|  5.1|     13.0|2012-11-22 05:35:03|11488 Weaver Cape...|Smith, Kim and Ma...|    1|[40.0,8283.32,1.0...|\n",
      "|     Harold Griffin|41.0|       6569.87|              1|  4.3|     11.0|2015-03-28 02:13:44|1774 Peter Row Ap...|Snyder, Lee and M...|    1|[41.0,6569.87,1.0...|\n",
      "|       James Wright|38.0|      10494.82|              1| 6.81|     12.0|2015-07-22 08:38:40|45408 David Path ...|      Sanders-Pierce|    1|[38.0,10494.82,1....|\n",
      "|      Doris Wilkins|45.0|       8213.41|              1| 7.35|     11.0|2006-09-03 06:13:55|28216 Wright Moun...|Andrews, Adams an...|    1|[45.0,8213.41,1.0...|\n",
      "|Katherine Carpenter|43.0|      11226.88|              0| 8.08|     12.0|2006-10-22 04:42:38|Unit 4948 Box 481...|Morgan, Phillips ...|    1|[43.0,11226.88,0....|\n",
      "|     Lindsay Martin|53.0|       5515.09|              0| 6.85|      8.0|2015-10-07 00:27:10|69203 Crosby Divi...|      Villanueva LLC|    1|[53.0,5515.09,0.0...|\n",
      "|        Kathy Curry|46.0|        8046.4|              1| 5.69|      8.0|2014-11-06 23:47:14|9569 Caldwell Cre...|Berry, Orr and Ca...|    1|[46.0,8046.4,1.0,...|\n",
      "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=[\"Age\",\"Total_Purchase\", \"Account_Manager\",\"Years\",\"Num_Sites\"],outputCol=\"Independent Features\")\n",
    "output=featureassembler.transform(training)\n",
    "output.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f395021a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Independent Features=DenseVector([42.0, 11066.8, 0.0, 7.22, 8.0]), Churn=1),\n",
       " Row(Independent Features=DenseVector([41.0, 11916.22, 0.0, 6.5, 11.0]), Churn=1),\n",
       " Row(Independent Features=DenseVector([38.0, 12884.75, 0.0, 6.67, 12.0]), Churn=1),\n",
       " Row(Independent Features=DenseVector([42.0, 8010.76, 0.0, 6.71, 10.0]), Churn=1),\n",
       " Row(Independent Features=DenseVector([37.0, 9191.58, 0.0, 5.56, 9.0]), Churn=1)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalized_data=output.select(\"Independent Features\",\"Churn\")\n",
    "finalized_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e92c9726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Independent Features=DenseVector([22.0, 11254.38, 1.0, 4.96, 8.0]), Churn=0),\n",
       " Row(Independent Features=DenseVector([27.0, 8628.8, 1.0, 5.3, 7.0]), Churn=0),\n",
       " Row(Independent Features=DenseVector([28.0, 8670.98, 0.0, 3.99, 6.0]), Churn=0),\n",
       " Row(Independent Features=DenseVector([28.0, 9090.43, 1.0, 5.74, 10.0]), Churn=0),\n",
       " Row(Independent Features=DenseVector([28.0, 11204.23, 0.0, 3.67, 11.0]), Churn=0)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "##train test split\n",
    "train_data,test_data=finalized_data.randomSplit([0.7,0.3])\n",
    "train_data.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6b25174",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=LogisticRegression(featuresCol='Independent Features', labelCol='Churn')\n",
    "regressor=regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d0c072dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0696, 0.0001, 0.7817, 0.5049, 1.2076])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b06795c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19.660683811712982"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Intercepts\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "df631108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Independent Features=DenseVector([25.0, 9672.03, 0.0, 5.49, 8.0]), Churn=0, rawPrediction=DenseVector([5.0034, -5.0034]), probability=DenseVector([0.9933, 0.0067]), prediction=0.0),\n",
       " Row(Independent Features=DenseVector([26.0, 8787.39, 1.0, 5.42, 11.0]), Churn=1, rawPrediction=DenseVector([0.609, -0.609]), probability=DenseVector([0.6477, 0.3523]), prediction=0.0),\n",
       " Row(Independent Features=DenseVector([26.0, 8939.61, 0.0, 4.54, 7.0]), Churn=0, rawPrediction=DenseVector([6.6579, -6.6579]), probability=DenseVector([0.9987, 0.0013]), prediction=0.0),\n",
       " Row(Independent Features=DenseVector([28.0, 11128.95, 1.0, 5.12, 8.0]), Churn=0, rawPrediction=DenseVector([4.1268, -4.1268]), probability=DenseVector([0.9841, 0.0159]), prediction=0.0),\n",
       " Row(Independent Features=DenseVector([29.0, 9378.24, 0.0, 4.93, 8.0]), Churn=0, rawPrediction=DenseVector([5.0226, -5.0226]), probability=DenseVector([0.9935, 0.0065]), prediction=0.0)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prediction\n",
    "pred_results=regressor.evaluate(test_data)\n",
    "pred_results.predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "064b0c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 FPR|                 TPR|\n",
      "+--------------------+--------------------+\n",
      "|                 0.0|                 0.0|\n",
      "|                 0.0| 0.00909090909090909|\n",
      "|                 0.0| 0.01818181818181818|\n",
      "|                 0.0| 0.02727272727272727|\n",
      "|                 0.0| 0.03636363636363636|\n",
      "|                 0.0|0.045454545454545456|\n",
      "|                 0.0| 0.05454545454545454|\n",
      "|                 0.0| 0.06363636363636363|\n",
      "|                 0.0| 0.07272727272727272|\n",
      "|                 0.0| 0.08181818181818182|\n",
      "|                 0.0| 0.09090909090909091|\n",
      "|                 0.0|                 0.1|\n",
      "|0.001897533206831...|                 0.1|\n",
      "|0.001897533206831...| 0.10909090909090909|\n",
      "|0.001897533206831...| 0.11818181818181818|\n",
      "|0.001897533206831...| 0.12727272727272726|\n",
      "|0.001897533206831...| 0.13636363636363635|\n",
      "|0.001897533206831...| 0.14545454545454545|\n",
      "|0.001897533206831...| 0.15454545454545454|\n",
      "|0.001897533206831...| 0.16363636363636364|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " regressor.summary.roc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0e90d80a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'Churn' given input columns: [Account_Manager, Age, Company, Independent Features, Location, Names, Num_Sites, Onboard_date, Total_Purchase, Years, prediction, probability, rawPrediction];\n'Project [prediction#3012, unresolvedalias(cast('Churn as double), None), 1.0 AS 1.0#3042]\n+- Project [Names#2945, Age#2946, Total_Purchase#2947, Account_Manager#2948, Years#2949, Num_Sites#2950, Onboard_date#2951, Location#2952, Company#2953, Independent Features#2965, rawPrediction#2978, probability#2993, UDF(rawPrediction#2978) AS prediction#3012]\n   +- Project [Names#2945, Age#2946, Total_Purchase#2947, Account_Manager#2948, Years#2949, Num_Sites#2950, Onboard_date#2951, Location#2952, Company#2953, Independent Features#2965, rawPrediction#2978, UDF(rawPrediction#2978) AS probability#2993]\n      +- Project [Names#2945, Age#2946, Total_Purchase#2947, Account_Manager#2948, Years#2949, Num_Sites#2950, Onboard_date#2951, Location#2952, Company#2953, Independent Features#2965, UDF(Independent Features#2965) AS rawPrediction#2978]\n         +- Project [Names#2945, Age#2946, Total_Purchase#2947, Account_Manager#2948, Years#2949, Num_Sites#2950, Onboard_date#2951, Location#2952, Company#2953, UDF(struct(Age, Age#2946, Total_Purchase, Total_Purchase#2947, Account_Manager_double_VectorAssembler_fc67f0bd8d23, cast(Account_Manager#2948 as double), Years, Years#2949, Num_Sites, Num_Sites#2950)) AS Independent Features#2965]\n            +- Relation [Names#2945,Age#2946,Total_Purchase#2947,Account_Manager#2948,Years#2949,Num_Sites#2950,Onboard_date#2951,Location#2952,Company#2953] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15796/1984425444.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnewdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_new.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureassembler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpred_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpred_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\challenge_cognodata\\lib\\site-packages\\pyspark\\ml\\classification.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dataset must be a DataFrame but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m         \u001b[0mjava_blr_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"evaluate\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumClasses\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mBinaryLogisticRegressionSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_blr_summary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\challenge_cognodata\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_call_java\u001b[1;34m(self, name, *args)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\challenge_cognodata\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\challenge_cognodata\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve 'Churn' given input columns: [Account_Manager, Age, Company, Independent Features, Location, Names, Num_Sites, Onboard_date, Total_Purchase, Years, prediction, probability, rawPrediction];\n'Project [prediction#3012, unresolvedalias(cast('Churn as double), None), 1.0 AS 1.0#3042]\n+- Project [Names#2945, Age#2946, Total_Purchase#2947, Account_Manager#2948, Years#2949, Num_Sites#2950, Onboard_date#2951, Location#2952, Company#2953, Independent Features#2965, rawPrediction#2978, probability#2993, UDF(rawPrediction#2978) AS prediction#3012]\n   +- Project [Names#2945, Age#2946, Total_Purchase#2947, Account_Manager#2948, Years#2949, Num_Sites#2950, Onboard_date#2951, Location#2952, Company#2953, Independent Features#2965, rawPrediction#2978, UDF(rawPrediction#2978) AS probability#2993]\n      +- Project [Names#2945, Age#2946, Total_Purchase#2947, Account_Manager#2948, Years#2949, Num_Sites#2950, Onboard_date#2951, Location#2952, Company#2953, Independent Features#2965, UDF(Independent Features#2965) AS rawPrediction#2978]\n         +- Project [Names#2945, Age#2946, Total_Purchase#2947, Account_Manager#2948, Years#2949, Num_Sites#2950, Onboard_date#2951, Location#2952, Company#2953, UDF(struct(Age, Age#2946, Total_Purchase, Total_Purchase#2947, Account_Manager_double_VectorAssembler_fc67f0bd8d23, cast(Account_Manager#2948 as double), Years, Years#2949, Num_Sites, Num_Sites#2950)) AS Independent Features#2965]\n            +- Relation [Names#2945,Age#2946,Total_Purchase#2947,Account_Manager#2948,Years#2949,Num_Sites#2950,Onboard_date#2951,Location#2952,Company#2953] csv\n"
     ]
    }
   ],
   "source": [
    "newdata = spark.read.csv('data_new.csv',header=True,inferSchema=True, sep=',')\n",
    "new_data = featureassembler.transform(newdata)\n",
    "pred_results=regressor.evaluate(new_data)\n",
    "pred_results.predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bac7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:challenge_cognodata] *",
   "language": "python",
   "name": "conda-env-challenge_cognodata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
